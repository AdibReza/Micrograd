# Micrograd

I created a simple autograd engine that handles both forward and backward propagation on a dynamically built DAG. On top of that, there's a small neural network library with a beginner-friendly PyTorch-like API. Throughout this project, I gained insights into:  

a) Understanding the inner workings of a neural network.  

b) Building one from scratch and realizing how straightforward it can be.  

c) Manually observing and tuning the loss and gradients for improved accuracy.  

d) Visualizing the neural network's complexity as it progresses through steps.

All the credits go to Andrej Karpathy.  
You can follow this link to do his course:https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=1
